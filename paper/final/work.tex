\chapter{Виды нейронных сетей.}

\section{Детерминированные нейронные сети.}

Сначала напомним, что такое обычные(детерминированные) нейронные сети и как они обучаются.

Основная задача обычных искусственных нейронных сетей($ANN$) в том, чтобы аппроксимировать некоторую зависимость выхода $y$ от
 входа $x$: $y = \Phi(x)$. Зависимость $\Phi(x)$ аппроксимируем через композицию последовательных преобразований.

Для простоты будем рассматривать обычные \textit{полносвязные} сети со входом $x$,
 скрытыми(промежуточными) состояниями слоёв $\bm{h_i}$, функциями активации $a_i(\cdot)$ и выходом $y$:
$$\bm{h_0} = \bm{x}$$
$$\bm{h_i} = a_i(\bm{W_i} \cdot h_{i-1} + \bm{b_i}), i = \overline{1...n}$$
$$\bm{h_n} = \widehat{y}$$
$$L = \mathcal{L}(\widehat{y}, y),$$ где $\mathcal{L}(\cdot, \cdot)$ - функция ошибки.

Обозначим параметры модели на $i$-ом слое $\bm{\theta_i} = (\bm{W_i}, \bm{b_i})$, а параметры всей модели через $\bm{\Theta} = \{\bm{\theta_i} :~i~=~\overline{1...n}\}$.
Чаще всего нейронные сети принято рассматривать, как вычислительный граф/граф вычислений.
 Такой подход удобен с инженерной точки зрения, поскольку позволяет воспользоваться инструментом автоматического
 дифференцирования, и используется во всех современных фреймоворках: PyTorch, TensorFlow и прочие.
 Граф вычислений является ациклическим ориентированным графом, составленным из вершин-переменных и вершин-операций(Рисунок~\ref{fig:image1}).
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{ANN.png}
    \caption{\textit{Полносвязная} сеть в виде графа вычислений}
    \label{fig:image1}
\end{figure}

Далее будем называть модели, основанные на графах вычислений, --- графовыми моделями. Графы вычислений могут разных типов:
 статическими/динамическими, детерминированными/вероятностными и т.д. Для обучения/настройки параметров детерминированных графовых моделей
 используется метод \textit{обратного распространения ошибки(back propagation)}, который широко используется в современном мире. Вкратце напомним алгоритм:

После прямого выполнения графа(\textit{forward pass}), то есть в соответствии с направлениями рёбер на выходе мы получаем
 $L$ -значение функции ошибки, которые в зависимости от задач мы хотим либо минимизировать, либо максимизировать. Для этого
 мы пользуемся градиентными методами оптимизации, что требует вычисление градиентов $\frac{dL}{d W_i}, \frac{dL}{d b_i}$ по нашим параметрам модели, где $i = \overline{1,n}$.
 В общем случае это трудная задача, однако в случае детерминированных графовых моделей мы можем использовать цепное правило(\textit{chain rule}) для того, чтобы последовательно
 проталкивать градиенты, начиная с концевой вершины, содержащей $L$.

 Например, для подсчёта градиентов $\frac{dL}{d W_i}, \frac{dL}{d b_i}$ мы представим его в виде
 $$\frac{dL}{d W_i} = \frac{dL}{d h_n} \cdot \frac{d h_n}{d W_i}$$
 $$\frac{dL}{d b_i} = \frac{dL}{d h_n} \cdot \frac{d h_n}{d b_i}$$

Аналогично для всех остальных параметров модели мы будем проталкивать накопленный с концевой вершины градиент до соответствующих вершин и
 с помощью этого градиента высчитывать градиент по параметрам модели. Схему работы алгоритма обратного распространения ошибки
 можно увидеть на Рисунок~\ref{fig:image2}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{ANN_back_prop.png}
    \caption{Обратное распространение ошибки по графу вычислений детерминированной полносвязной сети}
    \label{fig:image2}
\end{figure}

Однако детерминированные нейронные сети обладают несколькими проблемами:
\begin{itemize}
    \item Переобучение.
    \item Низкая интерпретируемость.
    \item Завышенная/заниженная уверенность модели в предсказаниях, даже если они неверные.
    \item Низкий уровень откалиброванности модели.
\end{itemize}

Указанные проблемы попытаемся решить с помощью байесовского подхода к нейронным сетям, который
 рассмотрим далее.

\section{Байесовские нейронные сети.}
\subsection{Вероятностные графы вычислений.}

Перед тем, как приступить к байесовским нейронным сетям, рассмотрим \textit{вероятностные графы вычислений}, на которых основаны
 байесовские сети. В литературе также часто вместо названия \textit{вероятностные графы вычислений} встречается \textit{вероятностные графические модели}.
 Второе название является более общим, в то время как первое более специфично именно для байесовских нейронных сетей.
 Такие графы вычислений широко используются и известны достаточно давно. Они лежат в основе, например,
 Марковских цепей, которые ранее активно использовались в различных задачах машинного предсказания, распознавания образов и т.п.

Основная мотивация в использовании вероятностного подхода состоит в том, что в реальном мире мы чаще имеем дело с неопределённостью в данных и знаниях
 и не можем детерминированно описать все приходящие переменные для решения задачи. Для решения проблем с неопределённостью
 можно попробовать собрать большие объёмы данных для того, чтобы попытаться "понять" эту неопределённость. С другой стороны
 мы можем использовать байесовский подход, который напрямую оперирует с неопределённостью.



\subsection{Байесовские нейронные сети.}
